---
title: "Mortgage Classification using CatBootclassifier"
author: "bokola"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  word_document:
    toc: yes
    toc_depth: '3'
  html_document:
    highlight: tango
    number_sections: yes
    theme: united
    toc: yes
    toc_depth: 3
---

# **Background**
## **Objective of the Model**

The objective is Predicting Mortgage Approvals From Government Data, taking into consideration the **demographics, location, property type, lender**, and other factors to predict whether a mortgage application was accepted or denied. It is a classification problem - A classifier is a machine learning model that separates the **label** into categories or **classes**. In other words, classification models are **supervised** machine learning models which predict a categorical label. We apply relevant skills to classify the label **`accepted`** - whether a mortgage application was accepted or declined using data obtained across the United States. 

## **Description of Data**
There are 23 variables (21 possible features and 1 binary label(`accepted`) and 1 arbitrary identifier(`row_id`). Both training and test sets span 500000 records

```{r packages, warning=FALSE,results='hide', echo=FALSE, message=FALSE}
rm(list = ls(all = T))
#setwd("C:\\Users\\bokola\\Google Drive\\Data")
ipk <- function(pkg){
  new.pkg <- list.of.pkg[!(list.of.pkg %in% installed.packages()[, "Package"])]
  if(length(new.pkg)) install.packages(new.pkg, dependencies = T)
  sapply(pkg, require, character.only = T)
  
}
list.of.pkg <- c("plyr", "tidyverse", "repr", "magrittr","kknn", "gridExtra", "GGally", "hexbin", "gridExtra", "caret", "ROCR", "pROC", "mice", "glmnet", "recipes", "h2o", "reticulate")
ipk(list.of.pkg)
options(repr.plot.width = 4, repr.plot.height = 3.5)

# h2o.no_progress()
# # launch h2o
# h2o.init()
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#knitr::opts_knit$set(root.dir = "C:\\Users\\bokola\\Google Drive\\Data")
hom.dir = ifelse(Sys.info()["sysname"] == "Windows", Sys.getenv("USERPROFILE"), Sys.getenv("HOME"))
project.path = path.expand(file.path(hom.dir
                                     ,"Analysis"
                                     ,"capstone")) %>% gsub("\\\\", "/", .)

data.path = path.expand(file.path(project.path
                                  ,"Data"))
scripts.path = path.expand(file.path(project.path
                                     ,"src"
                                     ,"r"))
report.path = path.expand(file.path(project.path
                                    ,"cache"
                                    ,"doc"))
if(!file.exists(project.path)){
  if(dir.create(project.path, recursive = T))
    stop("The project directory \"",
         project.path, "\"has been created!\nPlease fill it with relevant files and folders!")
  else
    stop("The project directory\"",
         project.path,
         "\"could not be created!")
}
if(!file.exists(data.path)){
  if(dir.create(data.path, recursive = T))
    stop("The project directory \"",
         data.path, "\"has been created!\nPlease fill it with relevant files and folders!")
  else
    stop("The project directory\"",
         data.path,
         "\"could not be created!")
}
if(!file.exists(report.path)){
  if(dir.create(report.path, recursive = T))
    stop("The project directory \"",
         report.path, "\"has been created!\nPlease fill it with relevant files and folders!")
  else
    stop("The project directory\"",
         report.path,
         "\"could not be created!")
}
if(!file.exists(scripts.path)){
  if(dir.create(scripts.path, recursive = T))
    stop("The project directory \"",
         scripts.path, "\"has been created!\nPlease fill it with relevant files and folders!")
  else
    stop("The project directory\"",
         scripts.path,
         "\"could not be created!")
}
```




```{r Data, warning=FALSE,results='hide', echo=FALSE, message=FALSE}
train_labels = read.csv(path.expand(file.path(data.path, "train_labels.csv")), header = T)
train_values = read.csv(path.expand(file.path(data.path, "train_values.csv")), header = T)
test_values = read.csv(path.expand(file.path(data.path, "test_values.csv")), header = T) 
train = merge(train_labels, train_values)

# distribution of NAs across variables
is_na = function(x) {
  for (col in names(x)) {
    #count = sum(which(x[,col] == "" | is.na(x[,col])))
    #count = sum(which(is.na(x[,col])))
    count = sum(is.na(x[,col]))
    cat(paste(col, ":", as.character(count), '\n'))
    
  }
}
is_na(train)
missing = c("applicant_income", "population", "minority_population_pct", "ffiecmedian_family_income", "tract_to_msa_md_income_pct", "number_of_owner.occupied_units", "number_of_1_to_4_family_units")
# train[, missing] = ifelse(is.na(train[,missing]), median(train[,missing]), train[, missing])
# train_fill = train[,missing]
# mice(data = train_fill, m = 5, method = "pmm", maxit = 5, seed = 500)
# train %<>% mutate_at(vars(applicant_income, tract_to_msa_md_income_pct), ~ifelse(is.na(.x), median(.x, na.rm = T), .x))
# test_values%<>% mutate_at(vars(applicant_income, tract_to_msa_md_income_pct), ~ifelse(is.na(.x), median(.x, na.rm = T), .x))
# dim(train) 
# head(train)

#Transformation
cols = c('msa_md', 'state_code','county_code','lender','loan_type','property_type', 'loan_purpose', 'occupancy','preapproval','applicant_ethnicity','applicant_race', 'applicant_sex')
train[,cols] = lapply(train[,cols], as.character)
test_values[,cols] = lapply(train[,cols], as.character)
#1. loan type
loan_type= c('1' = 'Conventional', '2' = 'FHA-insured ', '3' = 'VA-guaranteed', "4" = 'FSA/RHS')
property_type = c("1" = "One to four-family", "2" = "Manufactured housing", "3" = "Multifamily")
loan_purpose = c("1" = "Home purchase", "2" = "Home improvement", "3" = "Refinancing")
occupancy = c("1" = "Owner-occupied", "2" = "Not owner-occupied", "3" = "Not applicable")
preapproval = c("1" = "requested", "2" = "not requeste", "3" = "not applicable")
applicant_ethnicity = c("1" = "Hispanic or Latino", "2" = "Not Hispanic or Latino", "3" = "no information", "4" = "not applicable")#, "5" = "no co-applicant"
applicant_race = c("1" = "American Indian/Alaska Native", "2" = "Asian", "3" = "Black/African American", "4" =  "Native Hawaiian", "5" = "White", "6" = "no information", "7" = "not applicable")#, "8" = "no co-applicant"
applicant_sex = c("1" = "Male", "2" = "Female", "3" = "no information", "4" = "not applicable")

# out = rep('i', length.out = nrow(train))
# i=1
# for (x in train[,'loan_type']) {
#   out[i] = loan_type[[x]]
#   i=i+1
# }
# train[,"loan_type"] = out

for(var in c("loan_type","property_type","loan_purpose","occupancy", "preapproval", "applicant_ethnicity", "applicant_race", "applicant_sex")){
  train[,var]=sapply(train[,var],function(x) get(var)[[x]],simplify = T,USE.NAMES = F)
  #test_values[,var]=sapply(test_values[,var],function(x) get(var)[[x]],simplify = T,USE.NAMES = F)
}
a = list(train, test_values)

# v = function(y){
#   for(v in c("loan_type","property_type","loan_purpose","occupancy", "preapproval", "applicant_ethnicity", "applicant_race", "applicant_sex")){
#   #train[,var]=sapply(train[,var],function(x) get(var)[[x]],simplify = T,USE.NAMES = F)
#   y[,v]=sapply(y[,v],function(x) get(var)[[x]],simplify = T,USE.NAMES = F)
#   }
# }

for(v in c("loan_type","property_type","loan_purpose","occupancy", "preapproval", "applicant_ethnicity", "applicant_race", "applicant_sex")){
  #train[,var]=sapply(train[,var],function(x) get(v)[[x]],simplify = T,USE.NAMES = F)
  test_values[,v]=sapply(test_values[,v],function(x) get(v)[[x]],simplify = T,USE.NAMES = F)
}
#train[,cols] = lapply(train[,cols], as.factor)
#test_values[,cols] = lapply(train[,cols], as.factor)

#,"loan_purpose", "occupancy", "preapproval","applicant_ethnicity","applicant_race", "applicant_sex"
#ss=sapply(train[,var],function(x) get(var)[[x]],simplify = T,USE.NAMES = F)

# for(col in names(train)) {
#   if(is.character(train[, col])) {
#     count <- sum(ifelse(train[, col] == '-1'|is.na(train[,col]), 1, 0))
#     cat(paste(col, as.character(count), '\n'))
#   }
# }

```


There are a number of variables in the train and test sets with missing values: applicant_income - 39948, 
population - 22465, minority_population_pct - 22466, ffiecmedian_family_income - 22440, tract_to_msa_md_income_pct - 22514, number_of_owner.occupied_units - 22565, number_of_1_to_4_family_units - 22530. The missing entries were recoded to -999 for numeric variables.

## **Visualizing and imputing missing values**

```{r NAs, warning=FALSE,results='hide', echo=FALSE, message=FALSE}
p = train %>% 
  is.na() %>%
  reshape2::melt() %>%
  ggplot(aes(Var2, Var1, fill=value)) + 
    geom_raster() + 
    coord_flip() +
    scale_y_continuous(NULL, expand = c(0, 0)) +
    scale_fill_grey(name = "", labels = c("Present", "Missing")) +
    xlab("Observation") +
    theme(axis.text.y  = element_text(size = 4))
# vis_miss(train, cluster = TRUE)

```



```{r summaries, warning=FALSE,results='hide', echo=FALSE, message=FALSE}
train %>% summarise(min = min(loan_amount, na.rm = T), max = max(loan_amount, na.rm = T), median = median(loan_amount, na.rm = T), mean = mean(loan_amount, na.rm = T), sd = sd(loan_amount, na.rm = T))
train %>% aggregate(accepted ~ applicant_ethnicity, ., mean)
train %>% aggregate(accepted ~ applicant_sex, ., mean)
r = train %>% subset(., state_code == "48" & county_code != "-1") %>%  aggregate(accepted ~ county_code, ., mean)
range(r$accepted)
train %>% subset(., state_code %in% c("2","4")) %>% aggregate(accepted ~ state_code+loan_type, ., mean) %>% arrange(loan_type, accepted)

# plot_hist <- function(df, col = 'loan_amount', bins = 4){
#   #options(repr.plot.width = 4, repr.plot.height = 3.5)
#   #bw <- (max(df[, col]) - min(df[, col])) / (bins+1)
#   p <- ggplot(df, aes_string(col)) +
#     #geom_histogram(binwidth = bw) + scale_x_continuous(breaks = seq(0,1200, 200))#, aes(y = ..density..),alpha = 0.5) #+
#   geom_histogram()+ scale_x_log10(breaks = seq(0,1200, 100))#(breaks = seq(0,1200, 200))
#     #geom_density(aes(y = ..density..), color = 'blue') +
#     #geom_rug()
#   print(p)
# }
# plot_hist(train)
# df = subset(train, loan_amount <=1200)
# df = df %>% 
#   mutate
# (p = ggplot(subset(train, state_code == "45"), aes(x = applicant_income,y = loan_amount)) + geom_point() + geom_smooth())
  
```

# **Data Recoding and Visualization**

Variable recoding and visualization are key steps that ensure every variable is of the desired class/type. Visualizaation is key to understand the distribution and separation of values by the two levels of a binary label.

## **Data Recoding**

Our data has a number of categorical featuers (`msa_md`, `state_code`,`county_code`,`lender`,`loan_type`,`property_type`, `loan_purpose`, `occupancy`,`preapproval`,`applicant_ethnicity`,`applicant_race`, `applicant_sex`) but which are read in as numeric. We begin by recoding such variables into the desired character class and explicitly supplying the categories in both train and test sets. We then  visualize the separation of values between different levels of the label - `accepted(0,1)`.

## **Data Visualization**

1. **Class separation by numeric variables**

We explore the distinction of the distribution of values between the two levels of the label through boxplots.

```{r Data viz-numerics, warning=FALSE, echo=FALSE, message=FALSE}
 num_vars<-sapply(names(train),function(x) ifelse(is.numeric(train[,x]) & x !="row_id",x,NA))
 num_vars<-na.omit(num_vars)

 cat_vars<-sapply(names(train),function(x) ifelse(is.character(train[,x]),x,NA))
 cat_vars<-na.omit(cat_vars)

# cat_vars = function(x) {
#   x = sapply(names(x), function(x) ifelse(is.factor(train[,x]),x,NA))
#   x = na.omit(x)
#   
# } 

#train[, numeric_vars(train)] = sapply(train[, numeric_vars(train)], scale)
#test_values[, numeric_vars(test_values)] = sapply(test_values[, numeric_vars(test_values)], scale)
#ss=apply(train[, sapply(train, is.numeric)],2, scale)
train$accepted = as.factor(train$accepted)
plot_box <- function(df, cols, col_x = 'accepted'){
  options(repr.plot.width = 4, repr.plot.height = 3.5)
  for (col in cols){
    p <- ggplot(df, aes_string(col_x, col)) + 
        geom_boxplot() +
        ggtitle(paste('Box plot of', col, '\n vs.', col_x))
    print(p)
     
  }
}

num_var = c("loan_amount", "applicant_income", "population", "minority_population_pct", "ffiecmedian_family_income", "tract_to_msa_md_income_pct", "number_of_owner.occupied_units", "number_of_1_to_4_family_units")
plot_box(train, num_var)
```

From the plots, three variables - loan_amount, minority_population_pct and tract_to_msa_md_income_pct show clear separation of values between the two levels. Next we explore this separation by categorical variables.

1. **Class separation by categorical variables**

We explore the distinction of the distribution of values between the two levels of the label through bar plots.


```{r Data viz, warning=FALSE, echo=FALSE, message=FALSE}
plot_bars = function(df, catcols){
  options(repr.plot.width = 6, repr.plot.height = 5)
  temp0 = df[df$accepted == 0,]
  temp1 = df[df$accepted == 1,]
  for(col in cat_cols){
    p1 = ggplot(temp0, aes_string(col)) + 
      geom_bar() + 
      ggtitle(paste('Bar plot of \n', col, '\n for rejected loan applications')) + 
      theme(axis.text.x = element_text(angle = 90, hjust = 1))
    p2 = ggplot(temp1, aes_string(col)) + 
      geom_bar() + 
      ggtitle(paste('Bar plot of \n', col, '\n for approved loan applications')) + 
      theme(axis.text.x = element_text(angle = 90, hjust = 1))
    grid.arrange(p1, p2, nrow = 1)
  }
}
cat_cols = c("loan_type","property_type","loan_purpose","occupancy", "preapproval", "applicant_ethnicity", "applicant_race", "applicant_sex", "co_applicant")
plot_bars(train, cat_cols)
```

We observe the following:

1.	Some features such as property_type, loan_purpose, applicant_ethnicity, applicant_race and co_applicant have an almost significant different distribution of categories between the label categories.

2.	Others features such as preapproval, applicant_sex, occupancy show small differences, but these differences are unlikely to be significant.

Next we do feature engineering in readiness for modelling.


# **Feature Engineering**

Data preparation is an important step. We first **Scale** and **center** the numeric values of the features to ensure they have a similar range of values to avoid features with larger numeric values dominating model training. `Zscore` normalization is used. A glimpse of the numeric varibles is printed.

```{r feature engineering, warning=FALSE, echo=FALSE, message=FALSE}
#  num_vars<-sapply(names(train),function(x) ifelse(is.numeric(train[,x]) & x !="row_id",x,NA))
#  num_vars<-na.omit(numeric_vars)
# 
#  cat_vars<-sapply(names(train),function(x) ifelse(is.character(train[,x]),x,NA))
#  cat_vars<-na.omit(cat_vars)
# numeric_vars = function(x) {
#   x = sapply(names(x), function(x) ifelse(is.numeric(train[,x]) & x !="row_id" & x != "accepted",x,NA))
#   x = na.omit(x)
#   
# } 
# 
char_vars = function(x) {
  x = sapply(names(x), function(x) ifelse(is.character(train[,x]),x,NA))
  x = na.omit(x)

}

# train[, numeric_vars(train)] = sapply(train[, numeric_vars(train)], scale)
# test_values[, numeric_vars(test_values)] = sapply(test_values[, numeric_vars(test_values)], scale)

# scaling and centering numeric vars
# preProcValues = preProcess(train[, num_var], method = c("center", "scale"))
# train[, num_var] = predict(preProcValues, train[, num_var])
# test_values[, num_var] = predict(preProcValues, test_values[, num_var])

# test_values[, char_vars(test_values)] = lapply(test_values[, char_vars(test_values)], as.factor)
# train[, char_vars(train)] = lapply(train[, char_vars(train)], as.factor)

head(train[,num_var])


#ss=apply(train[, sapply(train, is.numeric)],2, scale)

#saving modified datasets
write.csv(train, file = path.expand(file.path(data.path, "train_preped.csv")), row.names = F)
write.csv(test_values, file = path.expand(file.path(data.path, "test_values_preped.csv")), row.names = F)


```
# **Model Building**

We construct a logistic model using `glm` function specifying the binomial distribution family. The formula used is: `**logistic_mod_1  = glm(accepted ~ loan_amount + property_type + loan_purpose + applicant_ethnicity + applicant_race + co_applicant + preapproval + applicant_sex + occupancy, family = binomial, data = train)**`


```{r modelling, warning=FALSE, echo=FALSE, message=FALSE}
train_complete = train[complete.cases(train),]
logistic_mod  = glm(accepted ~ loan_amount + minority_population_pct + tract_to_msa_md_income_pct + property_type + loan_purpose + applicant_ethnicity + applicant_race + co_applicant + preapproval + applicant_sex + occupancy, family = binomial, data = train)
logistic_mod_1  = glm(accepted ~ loan_amount + #minority_population_pct + tract_to_msa_md_income_pct 
                      + property_type + loan_purpose + applicant_ethnicity + applicant_race + co_applicant + preapproval + applicant_sex + occupancy, family = binomial, data = train)
logistic_mod$coefficients
logistic_mod_1$coefficients
test_values$probs <- predict(logistic_mod, newdata = test_values, type = 'response')
test_values$probs_1 <- predict(logistic_mod_1, newdata = test_values, type = 'response')
#test_values[1:20, c("row_id", "probs")]
test_values[1:20, c("row_id", "probs_1")]
# test_values %>% subset(., is.na(probs_1)) %>% nrow()
# test_values %>% subset(., probs_1 < .5) %>% nrow()
# train %>% subset(., accepted == 0) %>% nrow()
```

Logistic regression outputs log likelihoods with the class with the highest probability takes as the score(prediction)

```{r scoring, warning=FALSE, echo=FALSE, message=FALSE}
score_model <- function(df, threshold) {
  df$accepted <- ifelse(df$probs_1 < threshold, '0', '1')
  df
}
test <- score_model(test_values, 0.5)
test[1:5, c('row_id', 'accepted')]
write.csv(test[, c("row_id", "accepted")], file = path.expand(file.path(data.path, "predictions.csv")), row.names = F)
```

The initial model returns an accuracy of .51 indicating its poor performance in classifying the cases. We try `glmnet` approach next. As with regression, we perform some processing up-front to prepare for modelling with **glmnet**.We need to either ordinal encode or one-hot encode all categorical variables into numeric. Also, **glmnet** does not use the formula method (y ~ x) so we create our feature and target set and convert our features to a matrix.

```{r on-hot encoding, warning=FALSE, echo=FALSE, message=FALSE}
#one-hot encode our data with model.matrix
# full rank one-hot encode - recommended for glms and neural networks
#rm(train, train_labels, train_values, test_values, a, p, r, test, train_complete, logistic_mod, logistic_mod_1)
# rm(train_labels, train_values,  a, p, r, test, train_complete, logistic_mod, logistic_mod_1)
# 
# glm(accepted ~ loan_amount + minority_population_pct + tract_to_msa_md_income_pct + property_type + loan_purpose + applicant_ethnicity + applicant_race + co_applicant + preapproval + applicant_sex + occupancy, family = binomial, data = train)

# Set the response column to Sale_Price
response <- "accepted"
exclude <- c("accepted", "row_id")
# set the predictor names
predictors <- setdiff(colnames(train), exclude)

blueprint <- recipe(accepted ~ loan_amount + minority_population_pct + tract_to_msa_md_income_pct + property_type + loan_purpose + applicant_ethnicity + applicant_race + co_applicant + preapproval + applicant_sex + occupancy + ffiecmedian_family_income + number_of_owner.occupied_units + number_of_1_to_4_family_units + lender + msa_md + population, data = train) %>%
  step_medianimpute(loan_amount) %>%
  step_medianimpute(minority_population_pct) %>%
  step_medianimpute(tract_to_msa_md_income_pct) %>%
  step_integer(matches("property_type|loan_purpose|applicant_ethnicity|applicant_race|preapproval|applicant_sex|occupancy|lender|msa_md|county_code")) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) 
  #step_pca(all_numeric(), -all_outcomes())

prepare <- prep(blueprint, training = train)
baked_train <- bake(prepare, new_data = train)
baked_test <- bake(prepare, new_data = test_values)



logistic_mod_baked  = glm(accepted ~ loan_amount + minority_population_pct + tract_to_msa_md_income_pct + property_type + loan_purpose + applicant_ethnicity + applicant_race + co_applicant + preapproval + applicant_sex + occupancy + ffiecmedian_family_income + number_of_owner.occupied_units + number_of_1_to_4_family_units + lender + msa_md + population, family = binomial, data = baked_train)

test_values$probs_baked <- predict(logistic_mod_baked, newdata = baked_test, type = 'response')
test_values[1:20, c("row_id", "probs_baked")]


# full_rank <- caret::dummyVars(~ ., data = train, fullRank = F)
# train_1 <- predict(full_rank, train)
# test_1 <- predict(full_rank, test_values)
# 
# # less than full rank --> dummy encoding
# dummy <- dummyVars(~ ., data = train, fullRank = F)
# train_2 <- predict(dummy, train)
# test_2 <- predict(dummy, test_values)
```

```{r scoring-baked, warning=FALSE, echo=FALSE, message=FALSE}
score_model <- function(df, threshold) {
  df$accepted <- ifelse(df$probs_baked < threshold, '0', '1')
  df
}
test <- score_model(test_values, 0.51)
test[1:5, c('row_id', 'accepted')]
write.csv(test[, c("row_id", "accepted")], file = path.expand(file.path(data.path, "predictions_baked.csv")), row.names = F)
```

## **glmnet**

```{r on-hot encoding, warning=FALSE, echo=FALSE, message=FALSE}

#one-hot encode our data with model.matrix
one_hot <- model.matrix(~., train)[,-1] %>% as.data.frame()
names(one_hot)[names(one_hot) == "Attrition1"] <- "Attrition"
# create training and testing sets
set.seed(123)
split <- initial_split(one_hot, prop = 0.8, strata = "Attrition")
train <- training(split)
test <- testing(split)
#separate features from response var
train_x <- train %>% select(-Attrition) %>% as.matrix()
train_y <- train$Attrition
test_x <- test %>% select(-Attrition) %>% as.matrix()
test_y <- test$Attrition
# check that train & test sets have common response ratio
table(train_y) %>% prop.table()
table(test_y) %>% prop.table()
```






