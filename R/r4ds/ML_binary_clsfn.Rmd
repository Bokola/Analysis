---
title: "ML implementation in R"
author: "Basil"
date: "October 29, 2018"
output:
  pdf_document:
    toc: yes
    toc_depth: '3'
  html_document:
    highlight: tango
    number_sections: yes
    theme: united
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First, we install packages:

```{r, results='hide', message=FALSE}

ipk <- function(pkg) {
  new.pkg <- list.of.pkgs[!(list.of.pkgs %in% installed.packages()[,"Package"])]
  if(length(new.pkg)) install.packages(new.pkg,  dependencies = T)
  sapply(pkg, require, character.only = T)
}
# packages used
list.of.pkgs <- c("AmesHousing", "caret", "data.table", "dplyr", "ggplot2", "gbm", "glmnet", "h2o", "pdp",  "pROC", "purrr", "ranger", "ROCR", "rsample", "vip", "xgboost", "Rcpp", "forecast","car","VGAM")
ipk(list.of.pkgs)

# package and session infoR
sessionInfo()
```
# **Implementation: Binary Classification**
We use the employee attrition data to illustrate various regularization concepts for a binary classification problem, where the goal is to predict whether or not an employee attrits ("Yes" vs. "No").
It is advisable to recode a binary response variable into zeros and ones.
```{r}
attrition <- rsample::attrition %>%
  #mutate(Attrition = recode(Attrition, "Yes" = 1, "No" = 0)) %>%
  mutate_if(is.ordered, factor, ordered = FALSE)
levels(attrition$Attrition) <- c(0,1)
```
## **`glmnet'**
As with regression, we perform some processing up-front to prepare for modelling with **glmnet**.We need to either ordinal encode or one-hot encode all categorical variables into numeric. Also, **glmnet** does not use the formula method (y ~ x) so we create our feature and target set and convert our features to a matrix. Since our response var is imbalanced, we use `rsample` and `strat` to perform a stratified sampling so that our training and testing data have similar proportion of response levels.
```{r}
#one-hot encode our data with model.matrix
one_hot <- model.matrix(~., attrition)[,-1] %>% as.data.frame()
names(one_hot)[names(one_hot) == "Attrition1"] <- "Attrition"
# create training and testing sets
set.seed(123)
split <- initial_split(one_hot, prop = 0.8, strata = "Attrition")
train <- training(split)
test <- testing(split)
#separate features from response var
train_x <- train %>% select(-Attrition) %>% as.matrix()
train_y <- train$Attrition
test_x <- test %>% select(-Attrition) %>% as.matrix()
test_y <- test$Attrition
# check that train & test sets have common response ratio
table(train_y) %>% prop.table()
table(test_y) %>% prop.table()
```
### **Basic Implementation**
Similar to the regression problem, we apply a regularized classification model with `glmnet::glmnet()`. The difference is that for binary classification, we need to include `family = binomial`.
```{r}
# Apply ridge regression to attrition data
ridge = glmnet(
  x = train_x,
  y = train_y,
  family = "binomial",
  alpha = 0
)
plot(ridge, xvar = "lambda")
```
We can also access the coefficients for a model using `coef` and tidy the output with `tidy`. Here, we check out the top 10 largest absolute coefficient terms using the smallest and largest lambda values. We see that regardless of a large or small penalty parameter, working overtime and being a Sales Rep has the largest positive influence on the probability of attrition. Whereas being a Research Director and having high job involvement (among others) are the strongest influencers for reducing the probability of attrition.
```{r}
# Lambdas applied to penalty parameter
ridge$lambda %>% head()
# small lambda result in large coefficients
coef(ridge)[, 100] [-1] %>%
  tidy() %>%
  arrange(desc(abs(x)))
# large lambda result in small coefficients
coef(ridge)[, 1] [-1] %>%
  tidy() %>%
  arrange(desc(abs(x)))
```
At this point, we do not understand how much improvement we are experiencing in our loss function across various $\lambda$ values.

### **Tuning**
Recall: $\lambda$ is a tuning parameter that helps control our model from over-fitting to the training data. However, to identify the optimal $\lambda$ values, we need to perform cross-validation with `cv.glmnet`. Here we perform a 10-fold CV glmnet for both a ridge and lasso penalty.

By default, `cv.glmnet` uses deviance as the loss function for binomial classification but you could adjust `type.measure` to "auc", "mse", "mae", or "class" (missclassification).
```{r}
# apply cv ridge regression to attrition data
ridge <- cv.glmnet(
  x = train_x,
  y = train_y,
  family = "binomial",
  alpha = 0
)
# apply cv lasso regression to attrition data
lasso <- cv.glmnet(
  x = train_x,
  y = train_y,
  family = "binomial",
  alpha = 1
)
#plot results
par(mfrow = c(1,2))
plot(ridge, main = "Ridge penalty \n\n")
plot(lasso, main = "Lasso penalty \n\n")
```
Our plots above illustrate the 10-fold CV deviance across the $\lambda$ values. The ridge model doesn't in any way improve the loss function as $\lamba$ increases, however, the lasso model indicates a slight improvement in the deviance as our penalty $\lambda$ increases, suggesting that regular logistic regression model likely overfits our data. But as we constrain it further, our deviance starts to increase. The numbers at the top of the plot refer to the number of variables in the model. Ridge regression does not force any variables to exactly zero so all features will remain in the model but we see the number of variables retained in the lasso model go down as our penalty increases, with the optimal model containing between 38-48 predictor variables.
The first and second vertical dashed lines represent the  
$\lambda$ value with the minimum deviance and the largest  $\lambda$ value within one standard error of the minimum deviance
```{r}
# Ridge model
min(ridge$cvm) # min deviance
ridge$lambda.min # lambda for this min deviance
ridge$cvm[ridge$lambda == ridge$lamda.1se] # 1 st.error of min deviance
ridge$lambda.1se # lambda for this deviance
# Lasso model
min(lasso$cvm) # min deviance
lasso$lambda.min # lambda for this min deviance
lasso$cvm[lasso$lambda == lasso$lambda.1se] #1 st.error of min deviance
lasso$lambda.1se # lambda for this deviance
```
We can assses this visually. We plot the coefficients across the $\lambda$ values and the dashed red lin represents the $\lambda$ with the smallest deviance and the the dashed blue line represents largest $\lambda$ that falls within one standard error of the minimum deviance. This shows you how much we can constrain the coefficients while still maximizing predictive accuracy.
```{r}
# Ridge model
ridge_min <- glmnet(
  x = train_x,
  y = train_y,
  family = "binomial",
  alpha = 0
)

# Lasso model
lasso_min <- glmnet(
  x = train_x,
  y = train_y,
  family = "binomial",
  alpha = 1
)

par(mfrow = c(1, 2))
# plot ridge model
plot(ridge_min, xvar = "lambda", main = "Ridge penalty\n\n")
abline(v = log(ridge$lambda.min), col = "red", lty = "dashed")
abline(v = log(ridge$lambda.1se), col = "blue", lty = "dashed")

# plot lasso model
plot(lasso_min, xvar = "lambda", main = "Lasso penalty\n\n")
abline(v = log(lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(lasso$lambda.1se), col = "blue", lty = "dashed")
```
Above, we saw that both ridge and lasso penalties provide similiar minimum deviance scores; however, these plots illustrate that ridge is still using all 57 variables whereas the lasso model can get a similar deviance by reducing our feature set from 57 down to 48. However, there will be some variability with this MSE and we can reasonably assume that we can achieve a similar MSE with a slightly more constrained model that uses only 38 features. Although this lasso model does not offer significant improvement over the ridge model, we get approximately the same accuracy by using only 38 features! If describing and interpreting the predictors is an important outcome of your analysis, this may significantly aid your endeavor.
So far we've implemented a pure ridge and pure lasso model. However, we can implement an elastic net, by adjusting the `alpha` parameter. Any `alpha` value btwn 0-1 will perform an elastic net. When `alpha` = 0.5, we perform an equal combination of penalties whereas `alpha` $\rightarrow 0$ will have a heavier ridge penalty applied and `alpha` $\rightarrow 1$ will have a heavier lasso penalty.
```{r, elastic net}
lasso <- glmnet(train_x, train_y, family = "binomial", alpha = 1)
elastic1 <- glmnet(train_x, train_y, family = "binomial", alpha = 0.25)
elastic2 <- glmnet(train_x, train_y, family = "binomial", alpha = 0.75)
ridge <- glmnet(train_x, train_y, family = "binomial", alpha = 0)

par(mfrow = c(2,2), mar = c(6,4,6,2)+0.1)
plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n\n")
plot(elastic1, xvar = "lambda", main = "Elastic net (Alpha = .25)\n\n\n")
plot(elastic2, xvar = "lambda", main = "Elastic net (Alpha = .75)\n\n\n")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n\n")
```
Often, the optimal model contains `alpha` somewhere between 0-1, thus we want to tune both $\lambda$ and `alpha` parameters. As we did in the regression section, we create a common `fold_id`, which allows us to apply the same CV folds to each model. We then create a tuning grid that searches across a range of `alpha`s from 0-1, and empty columns where we'll dump our model results into.
```{r}
# maintain the same folds across all models
fold_id <- sample(1:10, size = length(train_y), replace = T)
# search across a range of alphas
tuning_grid <- tibble::tibble(
  alpha = seq(0,1, by = 0.1),
  dev_min =NA,
  dev_1se = NA,
  lambda_min = NA,
  lambda_1se = NA
)
#iterate over alpha value, however slower than the the h2o approach
for (i in seq_along(tuning_grid)) {
  fit <- cv.glmnet(
    x = train_x,
    y = train_y,
    family = "binomial",
    alpha = tuning_grid$alpha[i],
    foldid = fold_id
  )
  # extract MSE and lambda values
tuning_grid$dev_min[i] <- fit$cvm[fit$lambda == fit$lambda.min]
tuning_grid$dev_1se[i] <- fit$cvm[fit$lambda == fit$lambda.1se]
tuning_grid$lambda.min[i] <- fit$lambda.min
tuning_grid$lambda_1se[i] <- fit$lambda.1se
  
}
tuning_grid %>% broom::tidy()
```