---
title: "ML implementation in R"
author: "Basil"
date: "October 29, 2018"
output:   
  html_document:
    toc: True
    toc_depth: 3
    theme: united
    highlight: tango
    number_sections: True
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First, we install packages:

```{r, results='hide', message=FALSE}
ipk <- function(pkg) {
  new.pkg <- list.of.pkgs[!(list.of.pkgs %in% installed.packages()[,"Package"])]
  if(length(new.pkg)) install.packages(new.pkg,  dependencies = T)
  sapply(pkg, require, character.only = T)
}
# packages used
list.of.pkgs <- c("AmesHousing", "caret", "data.table", "dplyr", "ggplot2", "gbm", "glmnet", "h2o", "pdp",  "pROC", "purrr", "ranger", "ROCR", "rsample", "vip", "xgboost", "Rcpp", "forecast","car","VGAM")
ipk(list.of.pkgs)
# package and session info
sessionInfo()
```

Data:

```{r}
# access AmesHousing data
ames <- AmesHousing::make_ames()
# initial dimension 
dim(ames)
# response variable
head(ames$SalePrice)
# access _attrition_ data
attrition <- rsample::attrition
# initial dimensions
dim(attrition)
# response variable
head(attrition$Attrition)
# load training data https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz
train <- data.table::fread("C:/Users/admin/analysis/R/data/train.csv", data.table = FALSE)
# load test data https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz
test <- data.table::fread("C:/Users/admin/analysis/R/data/test.csv", data.table = FALSE)
# initial dimension
dim(train)
# response variable
head(train$V785)
```

# **Regularized Regression**
## **Implementation in `glmnet` **
We use the Ames, Ia housing data, with an intention to predicting `Sale_Price`

```{r}
# create training (70%) and test (30%) for the AmesHousing::make_ames() data
set.seed(123)
ames_split <- rsample::initial_split(AmesHousing::make_ames(), prop = 0.7, strata = "Sale_Price")
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```
`glmnet` requires that data is represented as a numeric matrix- `Matrix::sparse.model.matrix` for increased efficiency on large dim data.Also, parametric models are usuallly sensitive to skewed data, so you may want normalize your response variable first.

```{r}
# create training and testing feature matrices
# we use model.matrix(...)[,-1] to discard the intercept
train_x <- model.matrix(Sale_Price ~., ames_train)[,-1]
test_x <- model.matrix(Sale_Price ~., ames_test)[,-1]
# create training and testing response vectors
train_y <- log(ames_train$Sale_Price)
test_y <- log(ames_test$Sale_Price)
```
We use the `glmnet::g;mnet` function to apply a regularized model which:
  * standardizes features, a necessity when performing      regularized modelling. If you standardized your         predictors prior to **glmnet**, you can turn            `standardize = FALSE`
  * **glmnet** will perform ridge models accross a wide     range of ${\lambda}$ parameters, as illustrated         below:
```{r}
ridge <- glmnet(x = train_x, y = train_y, alpha = 0)
plot(ridge, xvar = "lambda")
ridge$lambda %>% head() # extracting values of lambda
```
We can also directly access the coefficients for a model using `coef`. **glmnet** stores all the coefficients for each model in order of largest to smallest ${\lambda}$.
```{r}
# small lambda results in large coefficients
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"), 100]
# large lambda result in small coefficients
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"), 1]
```
## **Tuning**
${\lambda}$ is a tuning parameter that helps control our model from over-fitting to the training data. To identify the optimal ${\lambda}$ value, we need to perform cross-validation (CV). `cv.glmnet` provides a built-in option to perform k-fold CV, and performs 10-fold CV by default. `cv.glmnet` uses MSE as the loss function but you can also use mean absolute error by changing the `type.measure` argument.
```{r}
# apply CV Ridge regression to Ames data
ridge <- cv.glmnet(
  x = train_x,
  y = train_y,
  alpha = 0
)
# apply CV lasso regression to Ames data
lasso <- cv.glmnet(
  x = train_x,
  y = train_y,
  alpha = 1
)
# plot results
par(mfrow = c(1, 2))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty \n\n")
```
First dotted vertical line in each plot represents the ${\lambda}$ with the smallest MSE and the second represents the ${\lambda}$ with an MSE within one standard error of the minimum MSE. The plots illustrate the 10-fold CV MSE across the ${\lambda}$ values. in both models we see a sligth improvement in the MSE as our penalty $(log {\lambda})$ gets larger, suggesting that a regular OLS model overfits our data. But as we constrain it further (continue to increase the penalty), our MSE starts to increase. The numbers at the top of the plot refer to the number of variables in the model. The first and the second vertical dashed lines represent the ${\lambda}$ value with the minimum MSE and the largest ${\lambda}$ within one standard error of the minimum MSE.
```{r}
# Ridge model
min(ridge$cvm) # min MSE
ridge$lambda.min # lambda for this min MSE
ridge$cvm[ridge$lambda == ridge$lambda.1se] # 1 st.error of min MSE
ridge$lambda.1se # lambda for this MSE

# lasso model
min(lasso$cvm) # min MSE
lasso$lambda.min # lambda for this min MSE
lasso$cvm[lasso$lambda == lasso$lambda.1se] # 1 st.error of min MSE
lasso$lambda.1se # lambda for this MSE
```

Above, we saw that both ridge and lasso penalties provide similiar MSEs; however, these plots illustrate that ridge is still using all 299 variables whereas the lasso model can get a similar MSE by reducing our feature set from 299 down to 131. However, there will be some variability with this MSE and we can reasonably assume that we can achieve a similar MSE with a slightly more constrained model that uses only 63 features. Although this lasso model does not offer significant improvement over the ridge model, we get approximately the same accuracy by using only 63 features! If describing and interpreting the predictors is an important outcome of your analysis, this may significantly aid your endeavor.
```{r}
# Ridge model
ridge_min <- glmnet(
  x = train_x,
  y = train_y,
  alpha = 0
)
# lasso model
lasso_min <- glmnet(
  x = train_x,
  y = train_y,
  alpha = 1
)

par(mfrow = c(1, 2))
# plot ridge model
plot(ridge_min, xvar = "lambda", main = "Ridge penalty\n\n")
abline(v = log(ridge$lambda.min), col = "red", lty = "dashed")
abline(v = log(ridge$lambda.1se), col = "blue", lty = "dashed")
# plot lasso model
plot(lasso_min, xvar = "lambda", main = "Ridge penalty\n\n")
abline(v = log(lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(lasso$lambda.1se), col = "blue", lty = "dashed")
```
An elastic net model can be implemented same way by selecting `alpha` value between 0-1. When `alpha = 0.5`, we perform an equal combination of penalties whereas alpha close to 0 will have a heavier ridge penalty applied and alpha close to 1 will have a heavier lasso penalty.
```{r}
lasso    <- glmnet(train_x, train_y, alpha = 1.0) 
elastic1 <- glmnet(train_x, train_y, alpha = 0.25) 
elastic2 <- glmnet(train_x, train_y, alpha = 0.75) 
ridge    <- glmnet(train_x, train_y, alpha = 0.0)

par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1)
plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n\n")
plot(elastic1, xvar = "lambda", main = "Elastic Net (Alpha = .25)\n\n\n")
plot(elastic2, xvar = "lambda", main = "Elastic Net (Alpha = .75)\n\n\n")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n\n")
```
Often the optimal model contains an alpha between 0-1, thus we want to tune both ${\lambda}$ and `alpha` parameters. To set up our tuning, we create a common `fold_id` which allows us to apply same CV folds to each model. We then create a tuning grid that searches across a range of `alpha`s from 0-1, and empty columns where we'll dump our model results into. Caution should when including ${\alpha = 0}$ or ${\alpha = 1}$ in the grid search. ${\alpha = 0}$ will produce a dense solution and it can be slow or impossible to compute in large N situations. ${\alpha = 1}$ has no $l_2$ penalty, therefore less numerically stable. It's recommended to search across ${\alpha}$ values of 0.1, .25, .5, .75, .9
```{r}
# maintain the same fold across all models
fold_id <- sample(1:10, size = length(train_y), replace = T)
# search across a range of alphas
tuning_grid <- tibble::tibble(
  alpha = seq(0,1, by = .1),
  mse_min = NA,
  mse_1se = NA,
  lambda_min = NA,
  lambda_1se = NA
)
```
We then iterate over each alpha value, apply a CV elastic net, and extract the minimum and one standard error MSE values and their respective ${\lambda}$ values.
```{r}
# perform grid search
for(i in seq_along(tuning_grid$alpha)) {
  # fit cv model for each alpha value
  fit <- cv.glmnet(train_x, train_y, alpha = tuning_grid$alpha[i], foldid = fold_id)
  # extract MSE and lambda values
  tuning_grid$mse_min[i] <- fit$cvm[fit$lambda == fit$lambda.min]
  tuning_grid$mse_1se[i] <- fit$cvm[fit$lambda == fit$lambda.1se]
  tuning_grid$lambda_min <- fit$lambda.min
  tuning_grid$lambda_1se <- fit$lambda.1se
}
tuning_grid
```
If we plot the MSE +/_ one standard error for the optimal ${\lambda}$ values for each `alpha` setting, we see that they fall within same level of accuracy. Consequently, we could select a full lassp model and gain the benefits of its feature selection capabilities and reasonably assume no loss in accuracy.
```{r}
tuning_grid %>%
  mutate(se = mse_1se - mse_min) %>%
  ggplot(aes(alpha, mse_min)) + 
  geom_line(size = 2) +
  geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = .25)
```