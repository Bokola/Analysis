---
title: "Hands-on Machine Learning with R"
author: "Basil"
date: "October 20, 2018"
output:
  html_document:
    highlight: tango
    number_sections: yes
    theme: united
    toc: yes
    toc_depth: 3
  word_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F)
```

We cover:

* Generalized low rank models
* Clustering algorithms
* Autoencoders
* Regularized models
* Random forests
* Gradient boosting machines
* Deep neural networks
* Stacking / super learner
* and more

We'll be using `glmnet`, `h20`, `ranger`, `xgboost` and `lime` packages.
So why R? The usefulness of R for data science can be attributed to its large and active ecosystem of third party packages: `tidyverse` for common data analysis activities; `h20`, `ranger`, `xgboost`, and more for scalable ML; `lime`, `pdp`, `DALEX`, and more for ML interpretability and much more that's not mentioned here.

First, we install packages:

```{r, results='hide', message=FALSE}
ipk <- function(pkg) {
  new.pkg <- list.of.pkgs[!(list.of.pkgs %in% installed.packages()[,"Package"])]
  if(length(new.pkg)) install.packages(new.pkg,  dependencies = T)
  sapply(pkg, require, character.only = T)
}
# packages used
list.of.pkgs <- c("AmesHousing", "caret", "data.table", "dplyr", "ggplot2", "gbm", "glmnet", "h2o", "pdp",  "pROC", "purrr", "ranger", "ROCR", "rsample", "vip", "xgboost", "Rcpp", "forecast","car","VGAM")
ipk(list.of.pkgs)
# package and session info
sessionInfo()
```

# **Chapter 1 Introduction **
The importance of ML can never be understated. Example application areas include:

  * predicting the likelihood of a patient returning to     the hospital (readmission) within 30 days of            discharge,
  * segmenting customers based on common attributes or      purchasing habits for target marketing,
  * predicting coupon redemption rates for a given          marketing campaign,
  * and much more.
The underpinning principle is that all ML tasks seek to learn from data. To address each question, we use a given set of *features* to train an algorithm and extract insights. The algorithms may be classified with regards to amount/type of *supervision* employed during training. Two main groups: ***supervised learners *** that are used to construct predictive models, and ***unsupervised learners*** that are used to build descriptive models. The type you will need to use depends on the task you hope to accomplish.

## **Supervised Learning **

A ***predictive model** is used for tasks that involve the prediction of a given output using other variables and their values (*features*) in the data set. It entails buliding mathematical models/tools that accurately predict an outcome. The learning algorithm in predictive models attempts to discover and model the relationship among the *target* response (the predicate) and other features (predictors)
Examples of predictive modelling include:

  * using customer attributes to predict the probability     of the customer churning in six weeks,
  * using home attributes to predict the sales price,
  * using employee attributes to predict the likelihood     of   attrition, etc.
Each of these examples have a defined learning task. They each intend to use attributes (***X***) to predict an outcome (***Y***).
The examples above describe *supervised learning*. The supervision refers to the fact that the target values provide a supervisory role, which indicates to the learner the task it needs to learn. Specifically given a set of data, the learning algorithm attempts to optimize a function (the algorithmic steps) to find the combination of feature values that result in a predicted value that is as close to the actual target output as possible.
In supervised learning, the training data you feed to the algorithm includes the desired solutions. Consequently, the solutions can be used to help *supervise* the training process to find the optimal algorithm parameters ( in terms of bias and model variance)
Supervised learning problems can be further classified into regression and classification tasks

### **Regression problems**
Predictions involving numeric outcome (should not be confused with linear regression modelling)

### **Classification Problems**
Involves predicting a categorical response. Revolve around predicting a binary or multinomial response measure such as:

* did a redeem a coupon (yes/no, 1/0),
* did a customer churn
* did a customer click on our online ad
* classifying customer reviews: 
    * binary: positive vis-a-vis negative
    * multinomial: extremely negative to extremely            positive on a 0-5 Likert scale
When we apply an ML algorithm to a classification problem, we often predict the probability of a particular class (i.e yes: 65, no:35) rather than prdicting a particular class (i.e., "yes" or "no")

## **Unsupervised Learning**

***Unsupervised learning***, unlike supervised learning includes a set of tools to better understand and describe your data but performs the analysis without a target variable. In essence, unsupervised learning is concerned with identifying groups in a dataset. The groups may be defined by the rows (i.e., *clustering*) or by the columns (i.e., *dimension reduction*).

In ***clustering***, observations are segmented into similar groups based on observed variables. An example is dividing consumers into homogenous groups (market segmentation). In ***dimension reduction***, the task is to reduce the number of variables in a data set. For example, classical regression models break down in the presence of highly correlated features. Dimension reduction techniques attempt to reduce the feature set to a potentially smaller set of uncorrelated variables. 
Unsupervised learning is usually done as part of exploratory data analysis.

### **The data sets**
1. Property sales information data
  * *problem type*: supervised regression
  * *response variable*: sale price(i.e. $195,000,         $215,000)
  * *features*: 80
  * *access*: provided by `AmesHousing` package
  * *more details*: see `?AmesHousing::ames_raw`

2. Employee attrition information
  * **problem type**: supervised binomial                  classification
  * **reponse variable**: `Attrition` (i.e. "Yes",         "No")
  * **features**: 30
  * **access**: provided by `rsample` package
  * **more details**: see `?rsample::attrition
  
3. Image information for handwritten data
  
  * **problem type**: supervised multinomial               classification
  * **response variable **: v785
  * **see the code chunk that follows for download         instructions 
  
  
  
  
```{r}
# access AmesHousing data

ames <- AmesHousing::make_ames()

# initial dimension 

dim(ames)

# response variable

head(ames$SalePrice)

# access _attrition_ data

attrition <- rsample::attrition

# initial dimensions

dim(attrition)

# response variable
head(attrition$Attrition)

# load training data https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz
train <- data.table::fread("C:/Users/basil mlis/analysis/R/data/train.csv", data.table = FALSE)

# load test data https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz
test <- data.table::fread("C:/Users/basil mlis/analysis/R/data/test.csv", data.table = FALSE)

# initial dimension
dim(train)


# response variable
head(train$V785)
```
  

# **Chapter 2: Preparing for Supervised Learning **

Machine learning is a very interative process; perform it correctly, you'll have great confidence in your outcome, otherwise you'll have useless results. Approaching machine learning correctly means approaching it strategically by wisely spending data on learning and validation procedures, properly pre-processing variables, minimizing data leakage, tuning hyperparameters, and assessing model performance.

## **Prerequisites**

The chapters utilizes the following packages:

```{r, message=F, warning=F}
ipk <- function(pkg) {
  new.pkg <- list.of.pkgs[!(list.of.pkgs %in% installed.packages()[,"Package"])]
  if(length(new.pkg)) install.packages(new.pkg, dependencies = T)
  sapply(pkg, require, character.only = T)
}
list.of.pkgs <- c("rsample", "caret", "h2o", "dplyr")
ipk(list.of.pkgs)
```

Initializing learning:

```{r}
# turn of progress bars
h2o.no_progress()
# launch h2o
h2o.init()
```

Since many of the supervised ML chapters leverage on the `h2o` package, we'll be showing how to do some tasks with the H2O objects. This requires your data to be an H2O object, done by supplying the `as.h2o()` function. 

**Tip**: Trying to convert a data set with ordered factors to an H2O object will throw an error, since H2O has no way of handling ordered factors. You must convert any ordered factors to unordered. 
 
```{r}
# ames data
ames <-AmesHousing::make_ames()
ames.h2o <- as.h2o(ames)
head(ames.h2o, n=2)

# attrition data
churn <- rsample::attrition %>%
  mutate_if(is.ordered, factor, ordered = F)
churn.h2o <- as.h2o(churn)
head(churn.h2o, n=2)
```
## **Data splitting **

### **Spending our data wisely **

A major goal of ML process is to find an algorithm  $f(x)$ that most accurately prdicts future values $(y)$ based on a set of inputs $(x)$. In other words, we want an algorithm that not only fits well to our past data, but more importantly one that predicts future values more accurately. This is referred to as the **generalizability** of an algorithm. How we spend our data will help us understand how well our algorithm generalizes to unseen data.
We split our data into training and test data sets to provide an accurate understanding of the generalizability of our final optimal model.

  * **Training set**: These data are used to train our     algorithms and tune hyperparameters
  * **Test set**: Having chosen a final model, these       data are used to estimate its prediction error         (generalization error). These data should not be       used during model training.

Given a fixed amount of data, typical recommendations for splitting your data into training - testing set include 60% (training) - 40% (testing), 70%-30%, or 80% - 20%. These are general guidline, keep in mind that as your overall data gets smaller:
  * Spending too much in training (> **80%**) won't        allow us to get a good assessment of predictive        performance. We may find a model that fits the         training data well but is not generalizable            (overfitting),
  * Sometimes too much spent in testing (> **40%**)        won't allow us to get a good assessment of model       parameters. Usually a 70-30 split is often             sufficient.
Two most common ways of splitting data include ***simple random sampling*** and ***stratified sammpling***

### **Simple random sampling**
SRS is one of the simplest ways to split the data into training and test sets. SRS does not control for any data attributes such as the % of data represented in your response variable $(y)$. Here we show four options to produce a 70-30 split(NB: setting the seed values allows you to reproduce your randomized splits):

```{r, split data}
# base
set.seed(123)
index_1 <- sample(1:nrow(ames), round(nrow(ames) * 0.7))
train_1 <- ames[index_1,]
test_1 <- ames[-index_1,]

# caret package
set.seed(123)
index_2 <- createDataPartition(ames$Sale_Price, p = 0.7, list = F)
train_2 <- ames[index_2,]
test_2 <-ames[-index_2,]

# rsample package
set.seed(123)
split_1 <- initial_split(ames, prop = 0.7)
train_3 <- training(split_1)
test_3 <- testing(split_1)

# h2o package
split_2 <- h2o.splitFrame(ames.h2o, ratios = 0.7, seed = 123)
train_4 <- split_2[[1]]
test_4 <- split_2[[2]]

```
Since this sampling approach will randomly sample across the distribution of $(y)$ (`Sale_Price` in our example), it will typically result into a similar distribution between your training and test sets as below shown:

```{r, plots}
library(ggplot2)
library(gridExtra)
plot_1 <- ggplot(data = train_1, aes(x = Sale_Price)) + geom_density(col = "black", trim = T) + geom_density(data = test_1, col = "red", trim = T) 

plot_2 <- ggplot(data = train_2, mapping = aes(x = Sale_Price)) + geom_density(col = "black", trim = T) + geom_density(data = test_2, col = "red", trim = T)

plot_3 <- ggplot(data = train_3, mapping = aes(x = Sale_Price)) + geom_density(col = "black", trim = T) + geom_density(data = test_3, col = "red", trim = T)

plot_4 <- ggplot(data = as.data.frame(train_4), mapping = aes(x = Sale_Price)) + geom_density(col = "black", trim = T) + geom_density(data = as.data.frame(test_4), col = "black", trim = T)

# piling the plots
gridExtra::grid.arrange(plot_1, plot_2, plot_3, plot_4, nrow =1)
```

### **Stratified sampling **
We can use stratified sampling if we want to explicitly control our sampling so that our training and test sets have similar $(y)$ distributions. This is more common with classification problems where the response variable may be imbalanced (90% of observations with response "yes" and 10% with response "No"). However, we can also apply it to regression problems for data sets that have a small sample size and where the response variable deviates strongly from normality. With a continuous response variable stratified sampling will break $y$ down into quantiles and randomly sample fro each quantile. The easiest way to achieve this is through the **rsample** package, where you specify the response variable to `strata` fy. A look at our employee attrition data indicates presence of an imbalanced response ( No: 84%, yes: 16%). By enforcing stratified sampling both our training and testing datsa sets have approximately equal response distributions.

```{r}
table(churn$Attrition) %>% prop.table()

# stratified sampling with rsample package
set.seed(123)
split_strat <- initial_split(churn, prop = 0.7, strata = "Attrition")
train_strat <- training(split_strat)
test_strat <- testing(split_strat)

# consistent response ratio between train & test
table(train_strat$Attrition) %>% prop.table()
table(test_strat$Attrition) %>% prop.table()
```

## **Feature engineering **
**Feature engineering refers to the process of adding, deleting, and transforming the variables to be applied to your ML algorithms. It is a significant process requiring that one spends time understanding their data. You can checkout  [this](http://shop.oreilly.com/product/0636920049081.do) for more on feature engineering

### **Response Transformation**
Normalizing the distribution of the response variable by using a *transformation* can lead to great improvements, especially for parametic models. As we saw in the data splitting section, our response variable `Sale_Price` is skewed to the right.

```{r}
ggplot(data = train_1, aes(x = Sale_Price)) + geom_density(col = "black", trim = T) + geom_density(data = test_1, col = "red", trim = T)
```

To normalize, we have a few options:
**Option1**: log transformation. This will transform most right skewed distributions to be approximately normal.

```{r, log_transform}
train_log_y <- log(train_1$Sale_Price)
test_log_y <- log(test_1$Sale_Price)
```
If your response has negative values then a log transformation will produce `NaN`s. If these negative values are small (between -0.99 and 0) then you can apply `log1p`, which adds 1 to the value prior to applying a log transformation. If your data consists of negatives <= -1, use the Yeo Johnson transformation.

**Option 2**: use a Box Cox transformation. A Box Cox transformation is more flexible and will find the transformation from a family of power transformations that will transform the variable as close as possible to the normal distribution.
**Note**: Be sure to compute the `lambda` on the training set and apply that same `lambda` to both the training and test set to  minimize data leakage.

```{r, Box Cox transform}
lambda <- forecast::BoxCox.lambda(train_1$Sale_Price)
train_bc_y <- forecast::BoxCox(train_1$Sale_Price, lambda)
test_bc_y <- forecast::BoxCox(train_1$Sale_Price, lambda)
```

A look into the transformations:
```{r}
plot_1 <-  ggplot(data = train_1, mapping = aes(x = Sale_Price)) + geom_histogram(bins = 50, fill = "red")
plot_2 <- ggplot(data = as.data.frame(train_log_y), mapping = aes(x = train_log_y)) + geom_histogram(bins = 50, fill = "green")
plot_3 <- ggplot(data = as.data.frame(train_bc_y), mapping = aes(x = train_bc_y)) +  geom_histogram(bins = 50, fill = "blue")
gridExtra::grid.arrange(plot_1, plot_2, plot_3, nrow = 1)
```

We can see that in this example, the log transformation and Box Cox transformation both perform equally well in transforming our response variable to be normally distributed.
**Note**: When you model with a transformed dependent variable, your predictions will also be in the transformed value. You will want to re-transform your predicted values back to their normal state so that decision makers can interpret the results.

```{r}
# log transform
y <- log(10)
# re-transform the log-transformed value
exp(y)

# Box Cox transform a value
y <- forecast::BoxCox(10, lambda)

# inverse Box Cox function
inv_box_cox <- function(x, lambda) {
  if(lambda==0)  exp(x) else (lambda*x + 1)^(1/lambda)
}
inv_box_cox(y, lambda)
```

**Tip**: If your response has negative values, you can use the Yeo-Johnson transformation. To apply, use `car::powerTransform` to identify the lambda, `car::yjPower` to apply the transformation, and `VGAM::yeo.johnson` to apply the transformation and/or the inverse transformation.

### **Predictor Transformation **

### **One-hot encoding **
Many models require all predictors to be numeric. As a consequence, we need to trannsform any categorical variables into numeric representations before applying the ML algorithms. This kind of recoding is automated in some packages (i.e. `h2o`, `glm`, `caret`) while others do not (`glmnet`, `keras`). There are other ways to recode categorical variables to numeric representations (i.e. one-hot, ordinal, binary, sum, Helmert). The most common is the one-hot encoding, where we transpose our categorical variables so that each level of the feature is represented as a boolean value. You could do a less than *full rank* encoding where we retain all variables for each level of `x`. However, this causes a perfect collinearity which causes problems with some ML algorithms (i.e. glm, neural networks). We can alternatively create full-rank one-hot encoding by dropping one of the levels in `x`. You can manually implement one-hot encoding with `caret::dummyVars`. Sometimes you may have a feature level with very few observations and all these observations show up in the test set but not the training set. The advantage of using `dumyVars` on the full data set and then applying the result to both the train and test data set is that it will guarantee that the same features are represented in both the train and test data.

```{r}
# full rank one-hot encode - recommended for glms and neural networks
full_rank <- caret::dummyVars(~ ., data = ames, fullRank = T)
train_oh <- predict(full_rank, train_1)
test_oh <- predict(full_rank, test_1)

# less than full rank --> dummy encoding
dummy <- dummyVars(~ ., data = ames, fullRank = F)
train_oh <- predict(dummy, train_1)
test_oh <- predict(dummy, test_1)
```

Two things to note:
  * Since one-hot encoding adds new features, it significantly increases the dimensionality of our data. You may        want to explore ordinal encoding if your data has categorical variables with many levels.
  * If using `h2o` you do  not need to explicitly encode your categorical predictors, you can insteaad override the     default encoding. This can be considered a tuning parameter as some encoding approaches will improve                modeling accuracy over other encodings. More on `h2o` encoding                                                      [here](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/categorical_encoding.html)

### **Standardizing**
Some models (K-NN, SVMs, PLS, neural networks) requiree that the predictor variables have the same units. **Centering** and **scaling** can be used for this purpose and is often referred to as ***standardizing*** the features. Standardizing numeric variables results in zero mean and unit variance, which provides a common comparable unit of measure across all the variables. Some packages have built-in arguments (i.e. `h2o`, `caret`) to standardize and some do not (i.e. `glm`, `keras`). You can manually standardize by supplying the `caret::preProcess` function. 
**Note**: It is best practice to standardize the test data based on the trainining mean and variance values of each feature. This minimizes data leakage. We center and scale our Ames predictor variables.

```{r}
# identify only the predictor vars
features <- setdiff(names(train_1), "Sale_Price")
# pre-process estimation based on training features
pre_process <- caret::preProcess(
  x = train_1[, features],
  method = c("center", "scale")
)

# apply to both training and test
train_x <- predict(pre_process, train_1[, features])
test_x <- predict(pre_process, test_1[, features])
```

### **Alternative feature Transformation **
There are some alternative transformations:
  * Normalizing the predictor variables with a Box Cox transformation can improve parametric model performance.
  * Collapsing highly correrated variables with PCA can reduce the number of features and increase the stability of     glm models. This has the downside of reducing amount of information at your disposal hence regularization is        usually a preferred altenative to PCA.
  * Removing near-zero or zero variance variables. Variables with very  little variance tend to not improve model       performance and can be removed. 
**Tip**: `preProcess` provides many other transformation options                                                  [here](https://topepo.github.io/caret/pre-processing.html) For example the code below normalizes predictors with a Box Cox transformation, center and scales coontinuous variables, performs PCA to reduce the predictor dimensions, and removes predictors with near zero variance.

```{r}
# identify only the predictor variables
features <- setdiff(names(train_1), "Sale_Price")

# pre-process estimation based on training features
pre_process <- preProcess(
  x      = as.data.frame(train_1[, features]),
  method = c("BoxCox", "center", "scale", "pca", "nzv")    
  )

# apply to both training & test
train_x <- predict(pre_process, as.data.frame(train_1[, features]))
test_x  <- predict(pre_process, as.data.frame(test_1[, features]))
```

## **Basic model formulation**
We explore a variety of packages and algorithms that perform and scale at best most orgainzation's problems and data sets. There are usually more than one way to approach an ML task. For example, the three functions below all produce the same linear regression output.

```{r}
lm.lm <- lm(Sale_Price ~., data = train_1) # ~. implies all features
lm.glm <- glm(Sale_Price ~., data = train_1, family = gaussian)
lm.caret <- train(Sale_Price ~., data = train_1, method = "lm")
```
An alternative model formulation approach is the *matrix formulation*. Matrix formulation requires that we separate our response variable from our features. For example in the regularization section, we'll use `glmnet` which requires our features (x) and respones (y) to be specified separately. Alternatively, `h2o` uses *variable name specification* to provide all the data combined in one `training_frame` but with features and response specified with character strings:

```{r}
# get feature names
features <- setdiff(names(train_1), "Sale_Price")
# create feature and response set
train_x <- train_1[, features]
train_y <- train_1$Sale_Price
# example matrix formulation
#glmnet.m1 <- train(x = train_x, y = train_y, method = "glmnet")

# create variable names and h2o training frame
y <- "Sale_Price"
x <- setdiff(names(train_1), y)
train.h2o <- as.h2o(train_1)
# example of variable name specification
h2o.m1 <- h2o.glm(x = x, y = y, training_frame = train.h2o)

```

## **Model tuning **
Hyperparameters control the level of model complexity. Tuning allows for model transformation to better align with patterns within our data. It is worth noting that highly tunable models can also be dangerous as they allow us to overfit our model to the training data, which will not generalize well to the future unseen data.One way to perform hyperparameter tuning is to to fiddle with hyperparameters manually until you find a great combination of hyperparameter values that result in high predictive accuracy. An alternative is to perform *grid search* - an automated approach to searching across many cominations of hyperparameter values.

## **Cross validation for generalization**
A model with a high *bias* is one that is rigid and consistent to a particular training set. If provided with a new training set, the model would not change much, if at all. Although consistent, the model does not capture underlying relationship. 
A model with a high *variance* is one that changes significantly with small changes to the training set. Our goal is to find a model that balances the ***bias-variance tradeoff***, by searching for a model that minimizes a k-fold cross-validation error metric. k-fold cross-validation is a resampling method that randomly divides the training data into k groups (folds) of approximately equal size. The model is fit on $k-1$ folds and then the held-out validation fold is used to compute the error. This procedure is repeated $k$ times; each time a different group of observations is treated as the validation set. The process results int k estimates for the test error. Thus the k-fold CV estimate is computed by averaging these values, which provides us with the error to expect on unseen  data. Algorithms explored herein all have built-in CV capabilities. An example is `h2o` that implements CV with the `nfolds` argument:

```{r}
# example of 10 fold CV in h2o
h2o.cv <- h2o.glm(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 10
)
```

## **Model evaluation**

### **Regression models**
  
  * **MSE**: Mean squared error, the average of the        squared error, along with RMSE are the most common     metrics to use. The squared commponent results in      larger errors having larger penalties. **Objective:     minimize**
  * **RMSE**: Root mean squared error takes the sqrt of     MSE so that your error is in the same units as your     response variable. **Objective: minimize**
  * **Deviance**: Short for mean residual deviance.        Provides a measure of goodness-of-fit of the model     being evaluated when compared to the null model        (intercept only). If the response var is gaussian,     then it equals MSE. When not, it usually gives a       more useful estimate of error. **Objective:            minimize**
  * **MAE**: Mean absolute error. Similar to MSE, but      takes absolute difference rather than squaring.        **Objective: minimize**
  * **RMSLE**: Root mean squared logarithmic error.        Similar to RMSE but performs a log() on the actual     and predicted values prior to computing the            difference. When your response variable has a wide     range of values, large response values with large      errors can dominate the MSE/RMSE metric. RMSLE         minimizes this impact so that small response values     with large errors can can have just as meaningful      of an impact as large response values with large       errors. **Objective: minimize**
   
   * **$R^2$**: Represents proportion of variance in       the dependent variable that is predictable from the     independent variable. Has a number of shortcomings     thus not much emphasis should be placed on it.         **Objective: maximize**

## **Classification models **
  
  * **Misclassification**: This is the overall error.      **Objective: minimize**
  * **Mean per class error **: This the average error      rate for each class. If your classes are balanced      this will be identical to misclassification.
  * **MSE**: Mean squared error. Computes the distance     from 1.0 to the probability suggested.                 **Objective: minimize**
  * **Cross-entropy (Log loss or Deviance)**: Similar      to MSE but incorporates a log of the predicted         probability multiplied by the true class. This         metric disproportionately punishes predictions         where we predict a small probability for the true      class. **Objective: minimize**
  * **Gini index**: Mainly used with tree-based methods     and commonly referred to as a measure of *purity*      where a small value indicates that a node contains     predominantly observations from a single class.       **Objective: minimize**
When applying classification models, we often use a *confusion* matrix to evaluate certain performance measures. A confusion matrix is a matrix that compares actual categorical levels to the predicted categorical levels.Includes observations like TP, TN, FP, FN.
  * **Accuracy**: Overall probability of correct           classification: **Objective: maximize**
  
    $(TP+TN)/total$
 
  * **Precision**: How accurately does the classifier      predict events: This metric is concerned with          maximizing the true positives to false positives       ratio **Objective: maximize**
    
    $TP/(TP+FP)$
  
  * **Sensitivity**: How accurately does the classifier      classify actual events? This metric is concerned       with maximizing the true positives to false            negatives ratio. **Objective: maximize**
      
      $TP/(TP+FN)$
  
  * **Specificity**: How accurately does the classifier     classify actual non-events? **Objective: maximize**
  
    $TN/(TN+FP)$
  
  * **AUC**: Area under curve. A good classifier will      have high precision and sensitivity. This implies a     classifier does well when it predicts an event will     and will not occur, which minimizes false negatives     and positives. To capture this balance, we often       use a ROC curve that plots the false positive rate     along the x-axis and the true positive rate along      the y-axis. A line that is diagonal from the lower     left corner to the upper right corner represents a     random guess. The higher the line is in the upper      left-hand corner, the better. AUC computes the area     under this curve. **Objective: maximize**

# **Regularized Regressions**
GLMs such as ODLS and logistic regressions are simple and fundamental approaches for supervised learning. Whenever assumptions required by GLMs are met, their models provide unbiased and low-variance model estimates. However, as the number of features grow, GLM assumptions typically break down, and the models often overfit (have high variance) to the training sample, causing an increase in sample errors in the test set. ***Regularization*** methods provide a means to control the coefficients, which can reduce the variance and decreaase hold-out sample errors.


